## <img src="../../../../../media/paste-c41beab67672608b9dafa2a68038dd64f1cab9ee.jpg">
<details>
<summary><b>Reveal answer</b></summary>
<h3><strong>Flashcard: Is Zero Initialization Good for Neural Networks?</strong></h3> <div><strong>Short Answer:</strong> ❌ <strong>No, it's a bad idea.</strong></div> <div><strong>Why?</strong></div> <ul> <li> <div>All neurons start the same → do the same computation.</div> </li> <li> <div>Get same gradients → update the same way (same error)</div> </li> <li> <div>❗ No learning diversity = wasted neurons.</div></li></ul>
</details>
